"""
Quiz Generator Module
=====================
Generate quiz questions from documents using RAG + configurable LLM backends.
Supports Ollama (local) and Groq Cloud (API) with strict JSON output.
"""

import logging
import json
import re
import xml.etree.ElementTree as ET
from xml.dom import minidom
from typing import List, Dict, Any, Optional
from datetime import datetime

from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

from .config import rag_config
from .retriever import DocumentRetriever
from .llm_providers import BaseLLM, LLMFactory

logger = logging.getLogger(__name__)


# ===== Quiz Data Models =====

class QuizQuestion(BaseModel):
    """Model for a single quiz question"""
    question: str = Field(description="Ná»™i dung cÃ¢u há»i")
    options: List[str] = Field(description="Danh sÃ¡ch 4 Ä‘Ã¡p Ã¡n A, B, C, D")
    correct_answer: str = Field(description="ÄÃ¡p Ã¡n Ä‘Ãºng (A, B, C hoáº·c D)")
    explanation: str = Field(description="Giáº£i thÃ­ch ngáº¯n gá»n táº¡i sao Ä‘Ã¡p Ã¡n Ä‘Ãºng")


class QuizOutput(BaseModel):
    """Model for quiz generation output"""
    quiz: List[QuizQuestion] = Field(default=[], description="Danh sÃ¡ch cÃ¢u há»i")
    message: str = Field(default="", description="ThÃ´ng bÃ¡o náº¿u cÃ³ lá»—i")


# ===== Prompt Template =====

QUIZ_GENERATION_PROMPT = """Báº¡n lÃ  má»™t giÃ¡o viÃªn chuyÃªn nghiá»‡p trong viá»‡c soáº¡n Ä‘á» thi tráº¯c nghiá»‡m cháº¥t lÆ°á»£ng cao.

NHIá»†M Vá»¤: Táº¡o {num_questions} cÃ¢u há»i tráº¯c nghiá»‡m dá»±a trÃªn ná»™i dung tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.

Ná»˜I DUNG TÃ€I LIá»†U (CONTEXT):
{context}

CHá»¦ Äá»€/YÃŠU Cáº¦U: {topic}

Äá»˜ KHÃ“: {difficulty} (easy/medium/hard)

QUY Táº®C Báº®T BUá»˜C:
1. CHá»ˆ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong Context Ä‘á»ƒ táº¡o cÃ¢u há»i
2. KHÃ”NG bá»‹a Ä‘áº·t hoáº·c thÃªm kiáº¿n thá»©c ngoÃ i tÃ i liá»‡u
3. Má»—i cÃ¢u há»i PHáº¢I cÃ³ Ä‘Ãºng 4 Ä‘Ã¡p Ã¡n: A, B, C, D
4. CÃ¡c Ä‘Ã¡p Ã¡n sai pháº£i há»£p lÃ½, khÃ´ng quÃ¡ dá»… loáº¡i trá»«
5. CÃ¢u há»i pháº£i rÃµ rÃ ng, khÃ´ng mÆ¡ há»“
6. TuÃ¢n thá»§ Ä‘á»™ khÃ³ yÃªu cáº§u:
   - easy: CÃ¢u há»i Ä‘Æ¡n giáº£n, kiá»ƒm tra ghi nhá»› cÆ¡ báº£n
   - medium: CÃ¢u há»i yÃªu cáº§u hiá»ƒu vÃ  Ã¡p dá»¥ng kiáº¿n thá»©c
   - hard: CÃ¢u há»i phÃ¢n tÃ­ch, so sÃ¡nh, tá»•ng há»£p thÃ´ng tin
7. TrÃ¡nh cÃ¢u há»i trÃ¹ng láº·p Ã½ nghÄ©a

HÆ¯á»šNG DáºªN Táº O CÃ‚U Há»ŽI CHáº¤T LÆ¯á»¢NG:
- CÃ¢u há»i kiá»ƒm tra hiá»ƒu biáº¿t, khÃ´ng chá»‰ ghi nhá»›
- ÄÃ¡p Ã¡n Ä‘Ãºng pháº£i chÃ­nh xÃ¡c theo tÃ i liá»‡u
- Giáº£i thÃ­ch ngáº¯n gá»n, trÃ­ch dáº«n tá»« context náº¿u cÃ³ thá»ƒ
- Náº¿u context khÃ´ng Ä‘á»§ Ä‘á»ƒ táº¡o {num_questions} cÃ¢u, táº¡o tá»‘i Ä‘a sá»‘ cÃ¢u cÃ³ thá»ƒ

Xá»¬ LÃ TRÆ¯á»œNG Há»¢P Äáº¶C BIá»†T:
- Náº¿u Context KHÃ”NG chá»©a thÃ´ng tin vá» "{topic}": tráº£ vá» quiz rá»—ng vá»›i message giáº£i thÃ­ch
- Náº¿u Context khÃ´ng Ä‘á»§ thÃ´ng tin: táº¡o Ã­t cÃ¢u hÆ¡n, KHÃ”NG bá»‹a

Äá»ŠNH Dáº NG OUTPUT (JSON):
{{
  "quiz": [
    {{
      "question": "Ná»™i dung cÃ¢u há»i?",
      "options": ["ÄÃ¡p Ã¡n A", "ÄÃ¡p Ã¡n B", "ÄÃ¡p Ã¡n C", "ÄÃ¡p Ã¡n D"],
      "correct_answer": "A",
      "explanation": "Giáº£i thÃ­ch ngáº¯n gá»n"
    }}
  ],
  "message": ""
}}

CHÃš Ã: Chá»‰ tráº£ vá» JSON, khÃ´ng thÃªm text khÃ¡c. Äáº£m báº£o JSON há»£p lá»‡."""


QUIZ_GENERATION_PROMPT_V2 = """You are an expert quiz creator. Create {num_questions} multiple-choice questions based ONLY on the provided document content.

DOCUMENT CONTENT:
{context}

TOPIC/REQUIREMENT: {topic}

DIFFICULTY LEVEL: {difficulty} (easy/medium/hard)

STRICT RULES:
1. Questions MUST be based ONLY on the provided content - DO NOT make up information
2. Each question has exactly 4 options: A, B, C, D
3. Wrong answers should be plausible but clearly incorrect based on the document
4. Follow the difficulty level:
   - easy: Simple recall questions testing basic facts
   - medium: Questions requiring understanding and application
   - hard: Complex questions requiring analysis and synthesis
5. Questions should test understanding, not just memorization
6. If content is insufficient, create fewer questions rather than inventing facts

OUTPUT FORMAT (JSON only, no markdown):
{{
  "quiz": [
    {{
      "question": "Question text here?",
      "options": ["Option A text", "Option B text", "Option C text", "Option D text"],
      "correct_answer": "A",
      "explanation": "Brief explanation why this is correct"
    }}
  ],
  "message": ""
}}

If the topic is not found in the document, return:
{{"quiz": [], "message": "KhÃ´ng tÃ¬m tháº¥y ná»™i dung vá» '{topic}' trong tÃ i liá»‡u"}}

Return ONLY valid JSON, no additional text."""


class QuizGenerator:
    """
    Generate quiz questions from documents using RAG.
    Supports multiple LLM providers with strict JSON output.
    """
    
    def __init__(
        self,
        retriever: DocumentRetriever,
        llm_provider: Optional[BaseLLM] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        base_url: Optional[str] = None
    ):
        """
        Initialize Quiz Generator.
        
        Args:
            retriever: DocumentRetriever instance
            llm_provider: Pre-configured LLM provider (if None, uses LLMFactory)
            model: Model name override (legacy, for backwards compatibility)
            temperature: Generation temperature (lower = more focused)
            base_url: API base URL (legacy)
        """
        self.retriever = retriever
        
        # Store legacy params for backwards compatibility
        self._model_override = model
        self._temperature_override = temperature if temperature is not None else 0.3
        self._base_url_override = base_url
        
        # Initialize LLM provider
        self._llm_provider: Optional[BaseLLM] = llm_provider
        
        # LLM instances (lazy initialized)
        self._llm = None  # Regular LLM
        self._llm_json = None  # JSON-mode LLM for quiz generation
        
        # Prompt templates
        self.prompt_vi = ChatPromptTemplate.from_template(QUIZ_GENERATION_PROMPT)
        self.prompt_en = ChatPromptTemplate.from_template(QUIZ_GENERATION_PROMPT_V2)
        
        # Initialize if provider not passed
        if self._llm_provider is None:
            self._init_llm()
        
        logger.info(f"QuizGenerator initialized with provider: {self._llm_provider.provider_name}")
    
    def _init_llm(self):
        """Initialize LLM using factory."""
        kwargs = {"temperature": self._temperature_override}
        if self._base_url_override:
            kwargs["base_url"] = self._base_url_override
        
        self._llm_provider = LLMFactory.create(
            model=self._model_override,
            **kwargs
        )
    
    @property
    def llm(self):
        """Get regular LLM instance (lazy initialization)."""
        if self._llm is None:
            self._llm = self._llm_provider.get_llm(json_mode=False)
        return self._llm
    
    @property
    def llm_json(self):
        """Get JSON-mode LLM instance for quiz generation (lazy initialization)."""
        if self._llm_json is None:
            self._llm_json = self._llm_provider.get_llm(json_mode=True)
        return self._llm_json
    
    @property
    def model(self) -> str:
        """Get current model name."""
        return self._llm_provider.model if self._llm_provider else self._model_override or rag_config.OLLAMA_MODEL
    
    def set_llm_provider(self, provider: BaseLLM):
        """
        Set a new LLM provider at runtime.
        
        Args:
            provider: New LLM provider instance
        """
        self._llm_provider = provider
        self._llm = None  # Reset cached instances
        self._llm_json = None
        logger.info(f"QuizGenerator LLM provider updated: {provider.provider_name}")
    
    def extract_topics_from_context(
        self,
        context: str,
        max_topics: int = 10
    ) -> Dict[str, Any]:
        """
        Extract topics from provided context using LLM.
        Used during document indexing to extract and cache topics.
        
        Args:
            context: Document content to analyze
            max_topics: Maximum number of topics to extract
            
        Returns:
            Dictionary with topics
        """
        logger.info("Extracting topics from provided context...")
        
        try:
            topic_prompt = ChatPromptTemplate.from_template("""Analyze the following document content and extract the main topics/concepts that could be used for quiz generation.

DOCUMENT CONTENT:
{context}

Extract {max_topics} main topics from this content. Topics should be:
- Specific enough to generate focused questions
- Clear and concise (1-5 words each)
- Represent key concepts, chapters, or sections in the document
- In the same language as the document content

OUTPUT FORMAT (JSON only):
{{
  "topics": [
    {{"name": "Topic Name", "description": "Brief description of what this topic covers"}}
  ]
}}

Return ONLY valid JSON, no additional text.""")
            
            # Use JSON-mode LLM for reliable JSON output
            chain = topic_prompt | self.llm_json
            
            response = chain.invoke({
                "context": context,
                "max_topics": max_topics
            })
            
            content = response.content if hasattr(response, 'content') else str(response)
            logger.info(f"Topic extraction response: {content[:300]}...")
            
            data = self._parse_quiz_response(content)
            
            if data and data.get("topics"):
                return {
                    "success": True,
                    "topics": data["topics"]
                }
            
            return {
                "success": False,
                "topics": [],
                "message": "Could not extract topics"
            }
            
        except Exception as e:
            logger.error(f"Error extracting topics from context: {e}")
            return {
                "success": False,
                "topics": [],
                "message": str(e)
            }
    
    def extract_topics(self, max_topics: int = 10) -> Dict[str, Any]:
        """
        Extract suggested topics from indexed documents using LLM.
        
        Args:
            max_topics: Maximum number of topics to suggest
            
        Returns:
            Dictionary with topics and metadata
        """
        logger.info("Extracting topics from documents...")
        
        try:
            # Get sample documents for topic extraction
            documents = self.retriever.vector_store.get_all_document_content(max_docs=20)
            
            if not documents:
                return {
                    "success": False,
                    "topics": [],
                    "message": "ChÆ°a cÃ³ tÃ i liá»‡u nÃ o Ä‘Æ°á»£c index"
                }
            
            # Create context from documents
            context = "\n\n---\n\n".join(documents[:15])  # Limit to avoid token overflow
            
            # Prompt for topic extraction - use JSON mode LLM
            topic_prompt = ChatPromptTemplate.from_template("""Analyze the following document content and extract the main topics/concepts that could be used for quiz generation.

DOCUMENT CONTENT:
{context}

Extract {max_topics} main topics from this content. Topics should be:
- Specific enough to generate focused questions
- Clear and concise (1-5 words each)
- Represent key concepts, chapters, or sections in the document

OUTPUT FORMAT (JSON only):
{{
  "topics": [
    {{"name": "Topic Name", "description": "Brief description of what this topic covers"}}
  ]
}}

Return ONLY valid JSON, no additional text.""")
            
            chain = topic_prompt | self.llm_json
            
            response = chain.invoke({
                "context": context[:8000],  # Limit context size
                "max_topics": max_topics
            })
            
            content = response.content if hasattr(response, 'content') else str(response)
            logger.info(f"Topic extraction response: {content[:300]}...")
            
            # Parse response
            data = self._parse_quiz_response(content)
            
            if data and data.get("topics"):
                topics = data["topics"]
                logger.info(f"Extracted {len(topics)} topics")
                return {
                    "success": True,
                    "topics": topics,
                    "message": ""
                }
            
            return {
                "success": False,
                "topics": [],
                "message": "KhÃ´ng thá»ƒ trÃ­ch xuáº¥t chá»§ Ä‘á» tá»« tÃ i liá»‡u"
            }
            
        except Exception as e:
            logger.error(f"Error extracting topics: {e}")
            return {
                "success": False,
                "topics": [],
                "message": f"Lá»—i: {str(e)}"
            }

    def generate_quiz(
        self,
        topic: str,
        num_questions: int = 5,
        difficulty: str = "medium",
        language: str = "vi",
        k: int = 10
    ) -> Dict[str, Any]:
        """
        Generate quiz questions based on a topic.
        
        Args:
            topic: Topic or description of what to quiz about
            num_questions: Number of questions to generate
            difficulty: Difficulty level - "easy", "medium", or "hard"
            language: "vi" for Vietnamese prompt, "en" for English
            k: Number of documents to retrieve for context
            
        Returns:
            Dictionary with quiz questions and metadata
        """
        logger.info(f"Generating quiz: topic='{topic}', num_questions={num_questions}, difficulty={difficulty}")
        
        # Step 1: Retrieve relevant documents
        documents = self.retriever.retrieve(topic, k=k)
        
        if not documents:
            logger.warning("No documents retrieved for topic")
            return {
                "success": False,
                "questions": [],
                "message": f"KhÃ´ng tÃ¬m tháº¥y ná»™i dung vá» '{topic}' trong tÃ i liá»‡u",
                "sources": []
            }
        
        # Step 2: Format context
        context = self.retriever.format_context(documents)
        
        if not context.strip():
            return {
                "success": False,
                "questions": [],
                "message": "Context rá»—ng, khÃ´ng thá»ƒ táº¡o quiz",
                "sources": []
            }
        
        # Step 3: Select prompt based on language
        prompt = self.prompt_vi if language == "vi" else self.prompt_en
        
        # Step 4: Generate quiz using JSON-mode LLM
        chain = prompt | self.llm_json
        
        try:
            logger.info("Generating quiz with LLM...")
            
            response = chain.invoke({
                "context": context,
                "topic": topic,
                "num_questions": num_questions,
                "difficulty": difficulty
            })
            
            # Parse response
            content = response.content if hasattr(response, 'content') else str(response)
            logger.info(f"Raw LLM response: {content[:500]}...")
            
            # Parse JSON
            quiz_data = self._parse_quiz_response(content)
            
            if not quiz_data:
                logger.error(f"Failed to parse quiz data from response")
                return {
                    "success": False,
                    "questions": [],
                    "message": "KhÃ´ng thá»ƒ parse káº¿t quáº£ tá»« LLM",
                    "sources": self.retriever.extract_citations(documents),
                    "raw_response": content
                }
            
            logger.info(f"Parsed quiz_data keys: {quiz_data.keys()}")
            logger.info(f"Number of quiz items: {len(quiz_data.get('quiz', []))}")
            
            # Check for error message from LLM
            if quiz_data.get("message") and not quiz_data.get("quiz"):
                return {
                    "success": False,
                    "questions": [],
                    "message": quiz_data["message"],
                    "sources": self.retriever.extract_citations(documents)
                }
            
            # Format quiz questions
            formatted_quiz = self._format_quiz(quiz_data.get("quiz", []))
            
            logger.info(f"Generated {len(formatted_quiz)} questions")
            
            if len(formatted_quiz) > 0:
                logger.info(f"Sample question 1: {formatted_quiz[0]}")
            
            return {
                "success": True,
                "questions": formatted_quiz,
                "message": quiz_data.get("message", ""),
                "sources": self.retriever.extract_citations(documents),
                "num_questions_requested": num_questions,
                "num_questions_generated": len(formatted_quiz)
            }
            
        except Exception as e:
            logger.error(f"Error generating quiz: {e}")
            return {
                "success": False,
                "questions": [],
                "message": f"Lá»—i khi táº¡o quiz: {str(e)}",
                "sources": self.retriever.extract_citations(documents),
                "error": str(e)
            }
    
    def _parse_quiz_response(self, content: str) -> Optional[Dict]:
        """Parse JSON response from LLM."""
        try:
            # Try direct JSON parse
            return json.loads(content)
        except json.JSONDecodeError:
            pass
        
        # Try to extract JSON from markdown code block
        json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', content)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass
        
        # Try to find JSON object in content
        json_match = re.search(r'\{[\s\S]*\}', content)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass
        
        logger.error(f"Could not parse JSON from response: {content[:200]}")
        return None
    
    def _format_quiz(self, quiz_list: List[Dict]) -> List[Dict]:
        """Format and validate quiz questions."""
        formatted = []
        
        for i, q in enumerate(quiz_list):
            try:
                # Validate required fields
                if not q.get("question"):
                    logger.warning(f"Question {i} missing 'question' field")
                    continue
                
                if not q.get("options"):
                    logger.warning(f"Question {i} missing 'options' field")
                    continue
                
                options = q.get("options", [])
                
                # Handle if options is already a dict
                if isinstance(options, dict):
                    # Already in {A: ..., B: ..., C: ..., D: ...} format
                    options_dict = options
                else:
                    # Convert list to dict
                    if not isinstance(options, list):
                        logger.warning(f"Question {i}: options is not list or dict: {type(options)}")
                        continue
                    
                    if len(options) != 4:
                        logger.warning(f"Question {i}: options has {len(options)} items, expected 4")
                        # Pad or trim to 4 options
                        while len(options) < 4:
                            options.append(f"ÄÃ¡p Ã¡n {chr(65 + len(options))}")
                        options = options[:4]
                    
                    options_dict = {
                        "A": options[0],
                        "B": options[1],
                        "C": options[2],
                        "D": options[3]
                    }
                
                # Get correct answer
                correct = q.get("correct_answer", "A").upper()
                if correct not in ["A", "B", "C", "D"]:
                    logger.warning(f"Question {i}: invalid correct_answer '{correct}', defaulting to 'A'")
                    correct = "A"
                
                formatted.append({
                    "question_number": i + 1,
                    "question": q["question"],
                    "options": options_dict,
                    "correct_answer": correct,
                    "explanation": q.get("explanation", "")
                })
                
                logger.debug(f"Formatted question {i+1}: {q['question'][:50]}...")
                
            except Exception as e:
                logger.warning(f"Error formatting question {i}: {e}")
                continue
        
        return formatted
    
    def generate_quiz_text(
        self,
        topic: str,
        num_questions: int = 5,
        k: int = 10
    ) -> str:
        """
        Generate quiz and return as formatted text.
        
        Args:
            topic: Topic to quiz about
            num_questions: Number of questions
            k: Documents to retrieve
            
        Returns:
            Formatted quiz text
        """
        result = self.generate_quiz(topic, num_questions, k)
        
        if not result["success"] or not result["quiz"]:
            return result.get("message", "KhÃ´ng thá»ƒ táº¡o quiz")
        
        lines = [f"ðŸ“ QUIZ: {topic.upper()}", "=" * 50, ""]
        
        for q in result["quiz"]:
            lines.append(f"CÃ¢u {q['id']}: {q['question']}")
            for letter, option in q["options"].items():
                lines.append(f"   {letter}. {option}")
            lines.append(f"   âœ… ÄÃ¡p Ã¡n: {q['correct_answer']}")
            if q.get("explanation"):
                lines.append(f"   ðŸ’¡ Giáº£i thÃ­ch: {q['explanation']}")
            lines.append("")
        
        lines.append("=" * 50)
        lines.append(f"Tá»•ng: {len(result['quiz'])} cÃ¢u há»i")
        
        return "\n".join(lines)
    
    def export_to_qti(
        self,
        questions: List[Dict[str, Any]],
        title: str = "Generated Quiz",
        description: str = ""
    ) -> str:
        """
        Export quiz questions to QTI 2.1 XML format.
        
        Args:
            questions: List of formatted quiz questions
            title: Quiz title
            description: Quiz description
            
        Returns:
            QTI XML string
        """
        # Create root element
        root = ET.Element('questestinterop')
        root.set('xmlns', 'http://www.imsglobal.org/xsd/ims_qtiasiv1p2')
        root.set('xmlns:xsi', 'http://www.w3.org/2001/XMLSchema-instance')
        root.set('xsi:schemaLocation', 'http://www.imsglobal.org/xsd/ims_qtiasiv1p2 http://www.imsglobal.org/xsd/ims_qtiasiv1p2p1.xsd')
        
        # Assessment
        assessment = ET.SubElement(root, 'assessment')
        assessment.set('ident', f'quiz_{datetime.now().strftime("%Y%m%d%H%M%S")}')
        assessment.set('title', title)
        
        # Metadata
        qtimetadata = ET.SubElement(assessment, 'qtimetadata')
        qtimetadatafield = ET.SubElement(qtimetadata, 'qtimetadatafield')
        ET.SubElement(qtimetadatafield, 'fieldlabel').text = 'qmd_timelimit'
        ET.SubElement(qtimetadatafield, 'fieldentry').text = '0'
        
        # Section
        section = ET.SubElement(assessment, 'section')
        section.set('ident', 'root_section')
        section.set('title', title)
        
        if description:
            ET.SubElement(section, 'rubric').text = description
        
        # Add questions
        for q in questions:
            item = ET.SubElement(section, 'item')
            item.set('ident', f'question_{q["question_number"]}')
            item.set('title', f'Question {q["question_number"]}')
            
            # Item metadata
            itemmetadata = ET.SubElement(item, 'itemmetadata')
            qtimetadata_item = ET.SubElement(itemmetadata, 'qtimetadata')
            
            # Question type
            field1 = ET.SubElement(qtimetadata_item, 'qtimetadatafield')
            ET.SubElement(field1, 'fieldlabel').text = 'question_type'
            ET.SubElement(field1, 'fieldentry').text = 'multiple_choice_question'
            
            # Points
            field2 = ET.SubElement(qtimetadata_item, 'qtimetadatafield')
            ET.SubElement(field2, 'fieldlabel').text = 'points_possible'
            ET.SubElement(field2, 'fieldentry').text = '1.0'
            
            # Presentation
            presentation = ET.SubElement(item, 'presentation')
            material = ET.SubElement(presentation, 'material')
            mattext = ET.SubElement(material, 'mattext')
            mattext.set('texttype', 'text/html')
            mattext.text = q["question"]
            
            # Response
            response = ET.SubElement(presentation, 'response_lid')
            response.set('ident', 'response1')
            response.set('rcardinality', 'Single')
            
            render_choice = ET.SubElement(response, 'render_choice')
            
            # Options
            for key, value in q["options"].items():
                response_label = ET.SubElement(render_choice, 'response_label')
                response_label.set('ident', key)
                mat = ET.SubElement(response_label, 'material')
                mat_text = ET.SubElement(mat, 'mattext')
                mat_text.set('texttype', 'text/plain')
                mat_text.text = value
            
            # Correct answer
            resprocessing = ET.SubElement(item, 'resprocessing')
            outcomes = ET.SubElement(resprocessing, 'outcomes')
            decvar = ET.SubElement(outcomes, 'decvar')
            decvar.set('maxvalue', '100')
            decvar.set('minvalue', '0')
            decvar.set('varname', 'SCORE')
            decvar.set('vartype', 'Decimal')
            
            # Correct response condition
            respcondition = ET.SubElement(resprocessing, 'respcondition')
            respcondition.set('continue', 'No')
            conditionvar = ET.SubElement(respcondition, 'conditionvar')
            varequal = ET.SubElement(conditionvar, 'varequal')
            varequal.set('respident', 'response1')
            varequal.text = q["correct_answer"]
            
            setvar = ET.SubElement(respcondition, 'setvar')
            setvar.set('action', 'Set')
            setvar.set('varname', 'SCORE')
            setvar.text = '100'
            
            # Feedback if explanation exists
            if q.get("explanation"):
                itemfeedback = ET.SubElement(item, 'itemfeedback')
                itemfeedback.set('ident', 'correct_fb')
                flow_mat = ET.SubElement(itemfeedback, 'flow_mat')
                material_fb = ET.SubElement(flow_mat, 'material')
                mattext_fb = ET.SubElement(material_fb, 'mattext')
                mattext_fb.set('texttype', 'text/html')
                mattext_fb.text = q["explanation"]
        
        # Convert to string with pretty print
        xml_str = ET.tostring(root, encoding='unicode')
        dom = minidom.parseString(xml_str)
        return dom.toprettyxml(indent='  ')
        
        return "\n".join(lines)
